{"version":1,"windowDimensions":{"x":0,"y":23,"width":1440,"height":805,"maximized":false},"grammars":{"grammarOverridesByPath":{}},"project":{"deserializer":"Project","paths":["/Users/igawataiichi/repo/report/20160720_機械学習_Bootcamp"],"buffers":[{"id":"452a0d28bbfce5226814fd9661881e67","text":"# ITOC 機械学習Bootcamp\n\n- 講師：大塚誠\n- 北海道釧路市出身\n\n# 1日目\n\n# 合宿のゴール\n- scikit-learnが使えるようになる。（デファクトスタンダード）\n  サイキットラーン\n\n＃ scikit-learn\n- Pythonの機械学習用ライブラリ\n\n# 機械学習とは？\n\n既存のデータの法則性を数理モデルに学習させ、\n未知のデータに渡して学習結果を汎化させるための枠組み\n\n- 既存のデータ\n- 予測モデル　★　これをつくることが目的\n- 予測の良さを測る定規\n- 学習アルゴリズム\n- 未知のデータ\n\n# モデルとは\n- モデル＝関数（数式）\n- モデルの複雑さ\n  - パラメータの複雑さ　＝　調整できるノブの数\n\n- パラメータ１つ\n  - y = a0 = f(x:a0)\n\n- パラメータ２つ\n  - y = a0 + a1X = f(x:a0,a1)\n      ↓背ペン　↓傾き\n\n- パラメータ3つ\n  - y=a0+a1x + a2x2=f(x:q0,q11,a2)\n\n\n# モデルの種類\n- 多項式モデル\n  - パラメータは無限に増やせる\n- オーバフィッテイング\n  複雑にフットさせすぎる\n\n# バイアス・バリアンストレードオフ\n\nバイアスとバリアンスが交わるところを狙うべし\n\n- バイアス(制約が強い)\n  - パラメータを増やすと、バイアス（制約）はへる。\n  - パラメータの数がおおいと、データの話を聞き過ぎる\n\n- バリアンス（散らばり）\n\n簡単           | 複雑\n:----------- | :----------\nIアンダーフィッティング | オーバーフィッティング\n\n\n# 機械学習の３本柱\n\n問題設定により、３つ別れれる。\n\n## 教師あり学習（ほとんどがコレ）95%\n- 分類\n  ものが、売れるか？売れないか？　連続しないやつ\n- 回帰\n  出力が連続になっている、グラフで表せる\n\n## 教師なし学習 4%\n- 分布推定\n\n  分布データの散らばりを予測する。\n  取れた魚からサンプルがほしいのではなくてい、この近海の海のサンプルがほしい\n\n- クラスタリング\n\n  分布推定ができれば、クラスタリングはできる。\n  山の数を数えればいいじゃん。\n\n- 次元圧縮\n  - 山の統合線ににている。（３次元になる）\n  - 重さ・長さの２次元だったら、１次元にしてもいいじゃんって感じ\n\n- 異常検知\n\n## 強化学習 1%\n\n- バンディット\n- 強化学習\n  コンピュータ動詞で戦わせて、学習させる。\n- 逆強化学習\n\n\nscikit-learn\n分類   | 回帰    | クラスタリング\n:--- | :---- | :------\n次元圧縮 | モデル選択 | 前処理\n\n\n## K分割交差検証(K=3の場合\n  cross_val_score\n  5つくらいがいいね\n  K=３で広く浅くする。\n  K=5 そこそこ正確にしたい場合\n  K=10　ホントに正確にしたい場合\n\n- データサイエンティストのいろは\n  しばたあきらさんの資料(p.21〜)\n  PyData Tokyo Tutorial & Hackathon #1\n\n- 機械学習コンペションにおける予測モデル手法\n  http://yukino.moo.jp/2016-07-16-DDBJ.pdf\n\n## 予測モデルの取出し方\n modleをシリアライズする。\n この中に、定規と学習アルゴリズム、予測モデルが入っている。\n\n## ハンズオン\n- numpy があるからpythonは人気、行列がとくい！！\n- しかもうらではC言語で動いているので早い\n\n# jupyter　notebook\nshift + tab\nshift + tab X 2 でメソッドの詳細を出してくれる\n\n% マジックコマンド　ipythonだけが必要\n`%matplotlib inline`\n上記を書くと、'matplotlib.pyplot'で図がかけるようになる。\n\n```python\nfig,ax = plt.subplots(figsize=(6,6))\n```\n\n- fig = ウィンドウ\n  フィグ\n- ax = divみたいなやつ\n  アクシス\n\n```python\nax.scatter\n```\n\n- すかったー\n２次元関数？\n↓　axis 0\n→  axis 1\n\n\n```python\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,test_size=.2, random_state=42)\n```\n\n- random_state を指定すると常に同じ値がかえってくる。\n- データはかならずシャッフルすべき\n- random_state 42 の理由\n- 銀河筆致俳句・ガイド　からきている\n\n## 線形モデル\n- linear\n\n- テストデータと、トラインデータが、差がないのはよいこと。\n\n\n## 決定木\n- [ensemble.RandomForestClassifier¶](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n\n## モデル選択\n\n２つの意味がある\n\n- モデルを選ぶ\n  - Decision Tree\n    - Random Foresetの木が１本だけのバージョン\n  - Random Foreset\n\n\n- ハイパーパラメータを選択する。\n\n## Pandas\n- なんでも数値にしてくれる　あるじゃん！\n\n\n##　サポートベクターマシーン\n## モデルアンサンブルによる予測\n\n\n# ２日目\n\n# Pandas のトピックス\n- DataFrame\n　表\n- Series\n\n行（横を取る）\n```python\ndf.iloc[1,:]\n```\n\n列（縦を取る）\n```python\ndf.ix[\"Japan\"]\n```\n\n- 平均はmean ミーン\n- Pandasは、StringをObjectとして表記する。\n\n# データフレーム\n- 欠損値を削除\n`df.dropna`\n\n\n[10分でパンダスを学ぶ](http://pandas.pydata.org/pandas-docs/stable/10min.html)\n[10分でパンダスを学ぶ　日本語](http://qiita.com/tkazusa/items/23bc0142bf277d397260)\n[チートシート](http://www.analyticsvidhya.com/blog/2015/07/11-steps-perform-data-analysis-pandas-python/)\n[よくつかう文法](http://qiita.com/okadate/items/7b9620a5e64b4e906c42)\n\n\n## 機械学習コンペティションにおける予測モデリング手法の傾向\n[資料](http://yukino.moo.jp/2016-07-06-DDBJ.pdf)\n\n1. 特徴設計・特徴選択\n  - なんのカラムを使うか\n2. 予測モデルの学習・チューニング\n  - サイキットラーン\n3. 予測結果出力\n\n- ロジスティック回帰の回帰は回帰の回帰じゃない。分類を示す。\n\n- 教師あり\n  - 分類\n    - サポートベクタマシン（SVM）\n      - 特徴\n        - デープラーニングのまえに流行っていた。\n        - すごいきれいにできている。\n        - 非線形分類モデルを効率気に行う。\n        - 自乗が入った瞬間に非線形\n      - 選択するときの判断材料\n        - 線形じゃないよね。非線形\n        - なんか丸い感じがする。\n      - カーネル\n        - gaussian kernal  = rvf\n\n    - ランダムフォレスト  (デフォルト１０この木)\n      - 特徴\n        - 予測モデルは人間が、みてもわかる。\n        - 決定きだから\n        - 複数の決定機を学習\n        - その際、入力例・変数の部分集合をランダムに選択\n        - 決定機の予測結果の平均値を最終出力とする\n\n      - 選択するときの判断材料\n        - 非線形\n        - 枠に張り付いて、分類する。\n\n    - 勾配(コウバイ)ブースティング（グレイデントブースティング）（デフォルト＝100の木）\n      - ランダムフォレスト　＋　勾配ブースティング\n      - ソフトウィエア：XGBoost\n      - 多数のコンペティションで良い成績\n      - これがでデファクトスタンダード\n\n    - ディープニューラルネットワーク(DNN)\n      - 多数の中間層の導入により複雑なおモデルを表現する\n      - TensorFlow(テンサフロー) がシェアNo１\n      - Theano(セアノ) PythonでGPUを使うときにつかう。\n        - Nビディアがトップになった。\n      - Chainer(チェイナー) 日本がつくった。\n      - keras(ケラス) セアノを使いやすくしたラッパー\n        - 最近日本でも人気がでてきた。\n        - tensorFowとTheanoを使える\n\n      [ニューラルネットワーウデモ](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.98269&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false)\n\n      - 最近はやっている、ハイパーパラメータの調整　ベイス最適化\n       - まだサイキットラーンには実装されてない。\n       - ガウシアンプロセス\n\n  - 特徴設計\n\n    １つひつの数値の意味が小さいのは、ディープランニングがとくい、（画像とか）\n    魚の長さや重さは、思い意味をもつので、ディープランニングは不得意\n\n    - データを過酷しやすい特徴を作り出す\n      - 良い特徴が明示的に与えられているとは限らない\n      - Kaggleの入力社は、多くの時間がを特徴設計にかけている\n    - 様ざなま表現を入力\n      - 文章での例：文字数、単語数、カンマの数、etc.\n      - 日付での例:年、月、曜日、祝日、etc.\n    - 特徴を組み合わせる：\n\n  - 特徴選択\n    - 特徴を選択して、もっともよい評価を導く\n    - 組み合わせてもOK\n\n＃スタッキング（ホールドアウト）\n  全ダインのモデルの洋装結果をメタ特徴そしてしよう\n\n\n# カテゴリの仕方\n- One-hot encodding\n 犬、猫、馬とかすくなかったらこれもでもよい。\n isDOc | isCat\n:---- | :----\n0     | 1\n1     | 0\n\n- One-hot encodding のコード\n```python\ndum = pd.get_dummies(df_train[\"Sex\"])\n```\n\n-　大量データの場合は、出現率を変換するなど\n- is_rareを作っていいね。\n\n## あやめのデータ・セット実際のデータ\n\n```python\ndict(zip(range(len(names)),names))\n```\n\nディスクライブ\n\ndum = pd.get_dummies(df_train[\"Sex\"])\ndf_train = pd.concat((df_train, dum), axis=1)\nX_train = df_train[['female','male']].as_matrix()\n\n\ndf_train['Age'] = df_train['Age']\ndf_train['Agefill':]\n\n- matplotlibは低レベル\n= seabornの読み込み　可視化用のライブラリ　シーボーン（）よくあるある分析をできる\n\n# 学んだこと\n- ランダムフォレストは、オーバーフィィットしやすい　（基の数、深さを調整する飛鳥がある）。\n- K分割交差ほうは、重要。\n- サーキットラーンには、numpy形式で渡す必要がある。\n- 欠損値は、平均値や、中央値を設定する。\n- 町長天を増やすと、ハイパーパラメータの再調整がいる。\n- サイキットラーンには、データフレームを渡すことはできない。(numpy.ndarrayのデータに変換する必要がある。as_matrixでね。)\n\n# ３日目\n- PPK（本多、遠藤、川島、井川）\n- 出雲大社の付近のお店の営業時間を伸ばす。\n- 島根に移住をする人をみつける。\n  - 定住したかどうか？のラベルを付ける良いかも\n\n# 気づき\n- 統計になってるよね。未知のデータとトレーニングデータを設定で考えて、ラベルYを考える必要ががある。\n","defaultMarkerLayerId":"0","markerLayers":{"0":{"id":"0","maintainHistory":false,"markersById":{},"version":2},"1":{"id":"1","maintainHistory":false,"markersById":{},"version":2},"2":{"id":"2","maintainHistory":true,"markersById":{"1":{"range":{"start":{"row":307,"column":9},"end":{"row":307,"column":9}},"properties":{"type":"selection","goalScreenRange":null,"preserveFolds":true,"autoscroll":false},"reversed":false,"tailed":false,"valid":true,"invalidate":"never"}},"version":2},"3":{"id":"3","maintainHistory":false,"markersById":{},"version":2},"7":{"id":"7","maintainHistory":false,"markersById":{},"version":2},"8":{"id":"8","maintainHistory":false,"markersById":{},"version":2},"12":{"id":"12","maintainHistory":false,"markersById":{},"version":2},"13":{"id":"13","maintainHistory":false,"markersById":{},"version":2},"17":{"id":"17","maintainHistory":false,"markersById":{},"version":2}},"nextMarkerLayerId":18,"history":{"version":3,"nextCheckpointId":2,"undoStack":[],"redoStack":[]},"encoding":"utf8","filePath":"/Users/igawataiichi/repo/report/20160720_機械学習_Bootcamp/report.md","digestWhenLastPersisted":"d06b567d255b1f7d8d6c3122e49ed99f87df9b10","preferredLineEnding":null,"nextMarkerId":223,"deserializer":"TextBuffer","version":5}]},"workspace":{"deserializer":"Workspace","paneContainer":{"deserializer":"PaneContainer","version":1,"root":{"deserializer":"Pane","id":3,"items":[{"deserializer":"TextEditor","id":8,"softTabs":true,"firstVisibleScreenRow":0,"firstVisibleScreenColumn":0,"displayBuffer":{"deserializer":"DisplayBuffer","id":9,"softWrapped":true,"tokenizedBuffer":{"deserializer":"TokenizedBuffer","bufferPath":"/Users/igawataiichi/repo/report/20160720_機械学習_Bootcamp/report.md","bufferId":"452a0d28bbfce5226814fd9661881e67","largeFileMode":false},"largeFileMode":false,"foldsMarkerLayerId":"1"},"selectionsMarkerLayerId":"2"}],"activeItemURI":"/Users/igawataiichi/repo/report/20160720_機械学習_Bootcamp/report.md","focused":true,"flexScale":1},"activePaneId":3},"packagesWithActiveGrammars":["language-gfm","language-python","language-hyperlink","language-todo"],"destroyedItemURIs":[]},"packageStates":{"linter":{"scope":"File"},"pigments":{"project":{"deserializer":"ColorProject","timestamp":"2016-07-15T15:19:06.379Z","version":"1.0.1","markersVersion":"1.0.5","globalSourceNames":["**/*.styl","**/*.stylus","**/*.less","**/*.sass","**/*.scss"],"globalIgnoredNames":["vendor/*","node_modules/*","spec/*","test/*"],"buffers":{"8":{"id":8,"path":"/Users/igawataiichi/repo/report/20160720_機械学習_Bootcamp/report.md","colorMarkers":[]}},"paths":[],"variables":{"deserializer":"VariablesCollection","content":[]}}},"symbol-gen":{},"symbols-tree-view":{},"term3":{"termViews":[]},"fuzzy-finder":{"/Users/igawataiichi/repo/report/20160720_機械学習_Bootcamp/report.md":1468244740048},"metrics":{"sessionLength":351206349},"tabs":[{}],"tree-view":{"directoryExpansionStates":{"/Users/igawataiichi/repo/report/20160720_機械学習_Bootcamp":{"isExpanded":true,"entries":{}}},"selectedPath":"/Users/igawataiichi/repo/report/20160720_機械学習_Bootcamp/report.md","hasFocus":false,"attached":true,"scrollLeft":0,"scrollTop":0,"width":200}},"fullScreen":false}